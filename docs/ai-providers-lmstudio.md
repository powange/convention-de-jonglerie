# Configuration de LM Studio pour l'IA

LM Studio est un outil desktop pour ex√©cuter des mod√®les de langage localement avec une interface graphique conviviale. Il expose une API compatible OpenAI, ce qui facilite son int√©gration.

## Avantages de LM Studio

- ‚úÖ **Interface graphique intuitive** - Pas besoin de ligne de commande
- ‚úÖ **Gratuit et open-source** - Pas de co√ªt par requ√™te
- ‚úÖ **Gestion simple des mod√®les** - T√©l√©chargement et chargement en 1 clic
- ‚úÖ **API compatible OpenAI** - Facile √† int√©grer
- ‚úÖ **Support GPU natif** - Performances optimis√©es automatiquement
- ‚úÖ **Confidentialit√© totale** - Donn√©es locales uniquement
- ‚úÖ **Monitoring en temps r√©el** - Voir les requ√™tes et r√©ponses

## Installation

### 1. T√©l√©charger LM Studio

Rendez-vous sur [lmstudio.ai](https://lmstudio.ai) et t√©l√©chargez la version pour votre syst√®me d'exploitation :

- **Windows** : LMStudio-win.exe
- **macOS** : LMStudio-mac.dmg
- **Linux** : LMStudio-linux.AppImage

### 2. Installer l'application

Suivez les instructions d'installation standard pour votre syst√®me.

## Configuration

### 1. T√©l√©charger un mod√®le avec support vision

LM Studio permet de t√©l√©charger facilement des mod√®les depuis Hugging Face.

**Mod√®les recommand√©s avec support vision** :

#### Option 1 : LLaVA (Recommand√© pour commencer)

- **Nom** : `xtuner/llava-llama-3-8b-v1_1-gguf`
- **Taille** : ~5-8 GB selon la quantification
- **RAM requise** : 8 GB minimum
- **Qualit√©** : Bonne
- **Vitesse** : Rapide

#### Option 2 : BakLLaVA

- **Nom** : `mys/ggml_bakllava-1`
- **Taille** : ~4 GB
- **RAM requise** : 6 GB minimum
- **Qualit√©** : Bonne
- **Vitesse** : Tr√®s rapide

#### Option 3 : LLaVA 13B (Pour de meilleures performances)

- **Nom** : `cjpais/llava-1.6-mistral-7b-gguf`
- **Taille** : ~8-12 GB
- **RAM requise** : 16 GB minimum
- **Qualit√©** : Excellente
- **Vitesse** : Moyenne

**Dans LM Studio** :

1. Ouvrir l'onglet "Search" (üîç)
2. Rechercher "llava" ou "bakllava"
3. Cliquer sur "Download" pour le mod√®le choisi
4. Choisir une quantification (Q4_K_M recommand√© pour un bon √©quilibre)

### 2. Charger le mod√®le

1. Aller dans l'onglet "Chat" (üí¨)
2. En haut, cliquer sur "Select a model"
3. Choisir le mod√®le LLaVA que vous avez t√©l√©charg√©
4. Attendre que le mod√®le se charge (barre de progression)

### 3. D√©marrer le serveur API

1. Aller dans l'onglet "Local Server" (üñ•Ô∏è)
2. Cliquer sur "Start Server"
3. Le serveur d√©marre par d√©faut sur `http://localhost:1234`
4. **Important** : Laisser LM Studio ouvert avec le serveur actif

### 4. Configurer l'application

Ajouter ces variables dans votre fichier `.env` :

```env
# Provider IA √† utiliser
AI_PROVIDER=lmstudio

# Configuration LM Studio
LMSTUDIO_BASE_URL=http://localhost:1234
LMSTUDIO_MODEL=auto
```

**Note** : `LMSTUDIO_MODEL=auto` utilise automatiquement le mod√®le charg√© dans LM Studio.

### 5. Red√©marrer l'application

```bash
# En d√©veloppement
npm run dev

# Ou avec Docker
npm run docker:dev:down && npm run docker:dev
```

## Configuration Docker

### Option 1 : LM Studio sur l'h√¥te (Recommand√©)

C'est l'approche la plus simple car LM Studio ne fonctionne pas bien en conteneur.

1. **Installer et d√©marrer LM Studio sur votre machine**
2. **Charger un mod√®le vision** (voir ci-dessus)
3. **D√©marrer le serveur API** dans LM Studio
4. **Configurer l'application** :

```env
AI_PROVIDER=lmstudio

# Windows/Mac (depuis Docker)
LMSTUDIO_BASE_URL=http://host.docker.internal:1234

# Linux (depuis Docker)
LMSTUDIO_BASE_URL=http://172.17.0.1:1234

LMSTUDIO_MODEL=auto
```

5. **Red√©marrer les conteneurs** :

```bash
npm run docker:dev:down && npm run docker:dev
```

## Utilisation

Une fois configur√©, l'utilisation est identique aux autres providers :

1. Acc√©der √† la page de gestion de l'√©dition
2. Section "Workshops" > "Importer depuis une photo"
3. S√©lectionner une image
4. LM Studio extraira automatiquement les workshops

## Monitoring et d√©bogage

### Voir les requ√™tes en temps r√©el

Dans LM Studio :

1. Aller dans l'onglet "Local Server"
2. Les requ√™tes s'affichent en temps r√©el avec :
   - Le prompt envoy√©
   - La r√©ponse g√©n√©r√©e
   - Le temps de traitement
   - Les tokens utilis√©s

### Logs de l'application

```bash
# Voir les logs Docker
npm run docker:dev:logs

# Ou en d√©veloppement local
# Les logs s'affichent dans le terminal
```

## Performances

### Sans GPU (CPU uniquement)

- **Temps de r√©ponse** : 15-45 secondes
- **RAM recommand√©e** : 16 GB
- **Mod√®le recommand√©** : LLaVA 8B en Q4_K_M

### Avec GPU NVIDIA

- **Temps de r√©ponse** : 5-15 secondes
- **VRAM recommand√©e** : 6+ GB
- **Mod√®le recommand√©** : LLaVA 13B en Q5_K_M

### Avec GPU Apple Silicon (Mac M1/M2/M3)

- **Temps de r√©ponse** : 8-20 secondes
- **RAM recommand√©e** : 16 GB
- **Mod√®le recommand√©** : LLaVA 8B en Q5_K_M
- **Note** : LM Studio optimise automatiquement pour Apple Silicon

## Param√®tres avanc√©s

### Ajuster les param√®tres dans LM Studio

Dans l'onglet "Local Server", vous pouvez ajuster :

- **Context Length** : Augmenter pour de longs prompts (d√©faut : 2048)
- **GPU Offload** : Nombre de couches sur GPU (auto recommand√©)
- **Temperature** : Cr√©ativit√© des r√©ponses (0.7 par d√©faut)

### Sp√©cifier un mod√®le particulier

Si vous avez plusieurs mod√®les charg√©s :

```env
LMSTUDIO_MODEL=xtuner/llava-llama-3-8b-v1_1-gguf
```

## Troubleshooting

### Erreur : "Service non accessible"

**Causes possibles** :

- LM Studio n'est pas d√©marr√©
- Le serveur API n'est pas actif dans LM Studio
- Mauvaise URL dans `LMSTUDIO_BASE_URL`

**Solutions** :

1. V√©rifier que LM Studio est ouvert
2. Aller dans "Local Server" et cliquer sur "Start Server"
3. Tester avec curl :
   ```bash
   curl http://localhost:1234/v1/models
   ```

### Erreur : "Aucun mod√®le charg√©"

**Solution** :

1. Aller dans l'onglet "Chat" de LM Studio
2. S√©lectionner un mod√®le avec support vision (LLaVA)
3. Attendre que le mod√®le se charge compl√®tement

### Performance tr√®s lente

**Solutions** :

- Utiliser un mod√®le plus petit (LLaVA 7B au lieu de 13B)
- Augmenter "GPU Offload" dans les param√®tres
- Fermer les autres applications gourmandes
- Utiliser une quantification plus agressive (Q4 au lieu de Q5)

### Erreur : "Out of memory"

**Solutions** :

- Utiliser un mod√®le plus petit
- R√©duire "Context Length" dans les param√®tres
- Fermer les autres applications
- Augmenter la RAM si possible

### R√©ponses de mauvaise qualit√©

**Solutions** :

- Utiliser un mod√®le plus gros (13B au lieu de 8B)
- Utiliser une quantification moins agressive (Q5 au lieu de Q4)
- Ajuster la temp√©rature dans LM Studio
- V√©rifier que l'image est claire et lisible

## Comparaison avec les autres providers

| Crit√®re             | Anthropic Claude | Ollama            | **LM Studio**     |
| ------------------- | ---------------- | ----------------- | ----------------- |
| **Interface**       | API uniquement   | Ligne de commande | **Interface GUI** |
| **Installation**    | Simple (cl√© API) | Moyenne           | **Tr√®s simple**   |
| **Gestion mod√®les** | N/A              | CLI               | **GUI 1 clic**    |
| **Monitoring**      | Dashboard web    | Logs              | **Temps r√©el**    |
| **Co√ªt**            | ~$0.01-0.03/img  | Gratuit           | **Gratuit**       |
| **Vitesse**         | ~3-5 sec         | ~5-60 sec         | ~8-20 sec         |
| **Pr√©cision**       | Excellente       | Bonne             | **Bonne**         |
| **Ressources**      | Aucune           | 8-16 GB RAM       | **8-16 GB RAM**   |

## Recommandations

### Pour le d√©veloppement

‚úÖ **LM Studio** - Interface intuitive, monitoring en temps r√©el, gratuit

### Pour la production avec faible volume

‚úÖ **LM Studio** - Gratuit, performances acceptables, confidentialit√©

### Pour la production avec fort volume

‚ö†Ô∏è **Anthropic** - Meilleure qualit√©, plus rapide, mais payant

## Ressources

- [Site officiel LM Studio](https://lmstudio.ai)
- [Documentation LM Studio](https://lmstudio.ai/docs)
- [Discord LM Studio](https://discord.gg/lmstudio)
- [Mod√®les Hugging Face](https://huggingface.co/models?pipeline_tag=image-text-to-text)

## Support

En cas de probl√®me :

1. Consulter les logs dans LM Studio (onglet "Local Server")
2. V√©rifier que le serveur API est actif
3. Tester avec curl : `curl http://localhost:1234/v1/models`
4. Consulter le Discord officiel LM Studio
5. Consulter cette documentation
